---
title: "How zomato manages it's logs?"
publishedAt: "2024-04-28"
summary: "How Zomato overcame the limitations of Elasticsearch and optimized their logging system to handle petabyte-scale data with ClickHouse."
tags: [databases, analysis, trends]
---

Well zomato, is not only India's biggest palyer in food delivery market but it also south asia's biggest food delivery aggregator, serves millions of users daily, resulting in an enormous volume of log data. With a peak log rate of 150 million events per minute and over 50 terabytes of uncompressed logs generated each day, maintaining efficient and scalable logging infrastructure is paramount. This article explores Zomato's transition from Elasticsearch to ClickHouse for managing their massive logging needs, detailing the challenges faced, the reasons for adopting ClickHouse, and the architectural and design choices that enabled the system's success.

### Challenges with elasticsearch

- **Scaling Difficulties:** Elasticsearch clusters struggled to keep up with the exponential growth in log data. The need for constant over-provisioning to handle traffic spikes led to increased operational complexity and cost. The Elasticsearch architecture requires horizontal scaling, which involves adding more nodes to the cluster, but the process is not always straightforward and can lead to data rebalancing issues.

<Callout emoji="ðŸ’¡">
  Zomato initially relied on Elasticsearch (ELK Stack) for their logging needs.
  However, as their platform scaled and traffic increased, several issues
  emerged.
</Callout>

- **Cost Implications:** The costs associated with maintaining and scaling Elasticsearch clusters became unsustainable. Managing variable traffic patterns required significant financial investment, impacting the overall cost-effectiveness of the logging system. Elasticsearch clusters require significant resources to handle indexing and querying large volumes of data, leading to high operational costs.
- **Performance Issues:** The performance of Elasticsearch degraded as the data volume grew, resulting in slower query times and increased ingestion lag. Elasticsearchâ€™s performance can be impacted by factors such as query complexity, data volume, and indexing strategy. As the dataset expanded, the system experienced increased latency in search and aggregation queries.

### Why ClickHouse?

To address these challenges, Zomato evaluated several alternatives before settling on ClickHouse. Hereâ€™s why ClickHouse emerged as the preferred solution:

- **High-Write Throughput:** ClickHouse's design supports high-speed data ingestion, making it capable of handling the massive log volumes generated by Zomato. It uses a multi-threaded architecture optimized for parallel processing, allowing for rapid ingestion of data.
- **Column-Oriented Storage:** Being a column-oriented database, ClickHouse optimizes read and write operations, enabling faster query performance and efficient data storage. Unlike row-oriented databases, columnar storage allows for efficient compression and retrieval of data, especially for analytical workloads.
- **Horizontal Scalability:** ClickHouseâ€™s shared-nothing architecture allows for seamless scaling by adding more nodes, which reduces operational overhead and improves fault tolerance. Each node operates independently, and data distribution is managed automatically, simplifying cluster management.
- **Efficient Compression:** ClickHouse employs advanced compression techniques to minimize storage space and enhance I/O efficiency, which is crucial for managing petabyte-scale data. ClickHouse uses algorithms like LZ4 and ZSTD for compression, which significantly reduce the storage footprint while maintaining high performance.

### Zomatoâ€™s logging system design

<Image
  src="https://lh7-us.googleusercontent.com/NL6Y3340UT23f97lPmAV-dhyH6PHBML1hkq3ifyVIDFkIs1WOVxEuivMhS1LbUiVy7bIuWfwsKBilysAc3RSOPtSD6uHm58g49_qvUQTkl_8SlZIlluNcVyOGRlDFUdSImh0CoJ5hFLtImnC5KxkyQY"
  alt="Zomato's logging system architecture"
/>

- **Filebeat:** Collects logs from Docker containers and EC2 instances. Filebeat is a lightweight shipper that forwards logs to Kafka, ensuring reliable and scalable log collection.

```yaml
filebeat.inputs:
  - type: log
    paths:
      - /var/log/*.log

output.kafka:
  hosts: ["kafka1:9092", "kafka2:9092"]
  topic: "logs"
```

- **Kafka:** Acts as a buffer for log data, ensuring reliable transmission to ClickHouse. Kafkaâ€™s distributed log storage and message queuing capabilities provide a fault-tolerant and high-throughput solution for log data streaming.
- **Custom Golang workers:** These workers process logs from Kafka and batch them before insertion into ClickHouse. The workers are designed to handle high throughput and perform transformations on log data to prepare it for storage.

```go
// Go code for processing and batching logs
package main

import (
    "github.com/segmentio/kafka-go"
    "github.com/ClickHouse/clickhouse-go"
)

func main() {
    // Connect to Kafka and ClickHouse
    kafkaReader := kafka.NewReader(kafka.ReaderConfig{ /* Kafka config */ })
    clickhouseConn, _ := clickhouse.OpenDirect("tcp://clickhouse:9000")

    // Process and batch logs
    for {
        msg, _ := kafkaReader.ReadMessage(context.Background())
        // Transform and batch logs
        clickhouseConn.Exec("INSERT INTO logs VALUES (?, ?, ?)", msg.Time, string(msg.Value), "INFO")
    }
}
```

### Data storage & ingestion

The log data is stored in ClickHouse, which is scaled to handle Zomato's requirements. Initially, the system utilized 10 M6g.16xlarge AWS EC2 nodes, but this configuration may evolve based on changing needs. The use of EC2 instances with high memory and compute capabilities ensures that ClickHouse can handle large volumes of data efficiently.

<Image
  src="https://lh6.googleusercontent.com/-q42plmXWjIfx61IuQbqQKWMkfCgEzid0eZ-eAzMdhW167bYFdCnZdg7VC8F5-qOBz83udtg_NYtPOKXURxv1roj4CATYQuRgeUyNRqAxH4MXFdU4vrCXXLwcdxu7pAYo3QxC1U5NQQYhnxFZmALcMk"
  alt="ClickHouse buckets"
/>

- **Batch Processing Ingestion:** Instead of using ClickHouse Kafka plugins, Zomato opted for custom Golang workers to batch log entries. Each batch contains up to 20,000 entries, reducing the overhead on ClickHouse and maintaining a lag of less than 5 seconds. Batch processing minimizes the number of write operations and optimizes the use of system resources.
- **Native Format Ingestion:** Data is inserted using ClickHouseâ€™s native format, resulting in a performance boost and reduced I/O intensity compared to HTTP-based insertion. Native format insertion leverages ClickHouseâ€™s optimized binary protocols for efficient data transfer.

```sql
-- Create a table for logs
CREATE TABLE IF NOT EXISTS logs (
    timestamp DateTime,
    message String,
    level LowCardinality(String)
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp);

-- Example of batch insertion using ClickHouseâ€™s native format
INSERT INTO logs (timestamp, message, level) VALUES
('2024-07-29 12:00:00', 'User logged in', 'INFO'),
('2024-07-29 12:01:00', 'Error processing request', 'ERROR'),
('2024-07-29 12:02:00', 'User logged out', 'INFO'),
('2024-07-29 12:03:00', 'System update completed', 'INFO');
```

- **Round-Robin load distribution:** Workers use a round-robin strategy to distribute load evenly across ClickHouse nodes, ensuring efficient data handling and redundancy. This approach helps balance the load and prevent bottlenecks in the system.

```go
// Pseudo-code for round-robin distribution
nodes := []string{"node1", "node2", "node3"} // List of ClickHouse nodes
currentNode := 0

for _, logEntry := range logEntries {
    node := nodes[currentNode]
    currentNode = (currentNode + 1) % len(nodes)

    // Insert data into the selected node
    insertIntoNode(node, logEntry)
}

func insertIntoNode(node string, logEntry LogEntry) {
    // Function to insert logEntry into the specified node
}
```

### Schema design in clickhouse

Logs are stored in **semi-structured tables** with common columns at the top level and other fields in a `Map<String, String>`. This design allows for flexible querying and efficient storage. The semi-structured approach facilitates easy adaptation to varying log formats and structures.

- **Compression Optimization:** The schema uses special codecs and `LowCardinality` strings to enhance compression ratios and reduce storage requirements. Compression codecs such as Delta, Gorilla, and LZ4 are employed to achieve high compression rates.

```sql
-- Example schema with semi-structured tables and compression optimization
CREATE TABLE logs
(
    timestamp DateTime,
    message LowCardinality(String),
    level LowCardinality(String)
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp);
```

- **Custom SDK:** Zomato developed an SDK to enforce structured logging practices, standardizing top-level fields to simplify log management and improve query efficiency. The SDK provides tools for consistent log formatting and integration with ClickHouse.

<u>**Secondary indexes:**</u> TokenBF_v1 Index significantly improves the query speed
bypassing non-matching data blocks, which is beneficial for large-scale data scans.
**TokenBF_v1** is a specialized index for efficiently handling high-cardinality data.

<Callout>
  Bloom filters were used to quickly determine if a query might match a set of
  records, reducing unnecessary data reads. These are probabilistic data
  structures that help in reducing the number of I/O operations.
</Callout>

```sql
-- Example of Bloom filter usage for optimizing query performance
CREATE TABLE logs
(
    timestamp DateTime,
    message String,
    level String
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp)
PRIMARY KEY (level)
INDEX idx_level (level) TYPE bloom_filter(0.1);
```

<u>**Query Throttling:**</u> Implements throttling mechanisms to handle high query
loads, prioritizing efficient queries and maintaining system availability. Throttling
ensures that the system can handle peak loads without compromising performance.

Optimizes resource allocation to prioritize write operations during system stress. The system dynamically adjusts resource allocation based on query types and workloads.

**Metrics collection:** Uses Prometheus to scrape ClickHouse metrics and Grafana for visualization and alerting. Prometheus collects time-series data on system performance, and Grafana provides dashboards for real-time monitoring.

> The migration to ClickHouse has resulted in performance enhancement for zomato

Achieved real-time data ingestion with less than 5 seconds of lag, supporting timely decision-making. The system's reliability has improved, allowing for near-instantaneous access to log data.

- **Performance:** Reduced query times to P99 of 10 seconds, with complex queries completing in under 20 seconds. Query performance has been significantly enhanced, making it easier to analyze large volumes of data.
- **Cost savings:** Potentially saved over a million dollars annually compared to the previous Elasticsearch setup. The cost-effectiveness of ClickHouse has resulted in substantial savings for Zomato.

## Conclusion

Zomato's transition to ClickHouse represents a strategic move to enhance their logging infrastructure, addressing scalability and cost issues associated with Elasticsearch. By migrating to ClickHouse's high-performance capabilities, custom design choices, and efficient data management practices, Zomato has significantly improved their logging system's reliability, performance, and cost-effectiveness.
