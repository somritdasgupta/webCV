---
title: "Why zomato switched from elasticsearch to clickhouse?"
publishedAt: "2024-04-28"
summary: "How Zomato overcame the limitations of Elasticsearch and optimized their logging system to handle petabyte-scale data with ClickHouse."
tags: [databases, analysis, trends]
---

![Zomato Scaling](https://zomatoblog.com/wp-content/uploads/2023/04/blog_header_db.png)

### Table of Contents

- [Introduction](#introduction)
- [Challenges with Elasticsearch](#challenges-with-elasticsearch)
- [Why ClickHouse?](#why-clickhouse)
- [Zomatoâ€™s Logging System Design](#zomatos-logging-system-design)
- [Improvements achieved](#improvements-achieved)
- [Conclusion](#conclusion)

---

## Introduction

Zomato, is not only India's biggest palyer in food delivery market but it also south asia's biggest food delivery aggregator, serves millions of users daily, resulting in an enormous volume of log data. With a peak log rate of 150 million events per minute and over 50 terabytes of uncompressed logs generated each day, maintaining efficient and scalable logging infrastructure is paramount. This article explores Zomato's transition from Elasticsearch to ClickHouse for managing their massive logging needs, detailing the challenges faced, the reasons for adopting ClickHouse, and the architectural and design choices that enabled the system's success.

### Challenges with elasticsearch

1. **Scaling Difficulties:** Elasticsearch clusters struggled to keep up with the exponential growth in log data. The need for constant over-provisioning to handle traffic spikes led to increased operational complexity and cost. The Elasticsearch architecture requires horizontal scaling, which involves adding more nodes to the cluster, but the process is not always straightforward and can lead to data rebalancing issues.
   <Callout emoji="ðŸ’¡">
     Zomato initially relied on Elasticsearch (ELK Stack) for their logging
     needs. However, as their platform scaled and traffic increased, several
     issues emerged.
   </Callout>
2. **Cost Implications:** The costs associated with maintaining and scaling Elasticsearch clusters became unsustainable. Managing variable traffic patterns required significant financial investment, impacting the overall cost-effectiveness of the logging system. Elasticsearch clusters require significant resources to handle indexing and querying large volumes of data, leading to high operational costs.

3. **Performance Issues:** The performance of Elasticsearch degraded as the data volume grew, resulting in slower query times and increased ingestion lag. Elasticsearchâ€™s performance can be impacted by factors such as query complexity, data volume, and indexing strategy. As the dataset expanded, the system experienced increased latency in search and aggregation queries.

### Why ClickHouse?

To address these challenges, Zomato evaluated several alternatives before settling on ClickHouse. Hereâ€™s why ClickHouse emerged as the preferred solution:

1. **High-Write Throughput:** ClickHouse's design supports high-speed data ingestion, making it capable of handling the massive log volumes generated by Zomato. It uses a multi-threaded architecture optimized for parallel processing, allowing for rapid ingestion of data.

2. **Column-Oriented Storage:** Being a column-oriented database, ClickHouse optimizes read and write operations, enabling faster query performance and efficient data storage. Unlike row-oriented databases, columnar storage allows for efficient compression and retrieval of data, especially for analytical workloads.

3. **Horizontal Scalability:** ClickHouseâ€™s shared-nothing architecture allows for seamless scaling by adding more nodes, which reduces operational overhead and improves fault tolerance. Each node operates independently, and data distribution is managed automatically, simplifying cluster management.

4. **Efficient Compression:** ClickHouse employs advanced compression techniques to minimize storage space and enhance I/O efficiency, which is crucial for managing petabyte-scale data. ClickHouse uses algorithms like LZ4 and ZSTD for compression, which significantly reduce the storage footprint while maintaining high performance.

### Zomatoâ€™s logging system design

![Zomato Loggin Sytsem Architecture](https://lh7-us.googleusercontent.com/NL6Y3340UT23f97lPmAV-dhyH6PHBML1hkq3ifyVIDFkIs1WOVxEuivMhS1LbUiVy7bIuWfwsKBilysAc3RSOPtSD6uHm58g49_qvUQTkl_8SlZIlluNcVyOGRlDFUdSImh0CoJ5hFLtImnC5KxkyQY)

### Log collection

Zomato's logging infrastructure involves several key components:

1. **Filebeat:** Collects logs from Docker containers and EC2 instances. Filebeat is a lightweight shipper that forwards logs to Kafka, ensuring reliable and scalable log collection.

```
# Example Filebeat configuration (filebeat.yml)
filebeat.inputs:
- type: log
  paths:
    - /var/log/*.log

output.kafka:
  hosts: ["kafka1:9092", "kafka2:9092"]
  topic: "logs"
```

2. **Kafka:** Acts as a buffer for log data, ensuring reliable transmission to ClickHouse. Kafkaâ€™s distributed log storage and message queuing capabilities provide a fault-tolerant and high-throughput solution for log data streaming.

3. **Custom Golang workers:** These workers process logs from Kafka and batch them before insertion into ClickHouse. The workers are designed to handle high throughput and perform transformations on log data to prepare it for storage.

```
// Example Go code for processing and batching logs
package main

import (
    "github.com/segmentio/kafka-go"
    "github.com/ClickHouse/clickhouse-go"
)

func main() {
    // Connect to Kafka and ClickHouse
    kafkaReader := kafka.NewReader(kafka.ReaderConfig{ /* Kafka config */ })
    clickhouseConn, _ := clickhouse.OpenDirect("tcp://clickhouse:9000")

    // Process and batch logs
    for {
        msg, _ := kafkaReader.ReadMessage(context.Background())
        // Transform and batch logs
        clickhouseConn.Exec("INSERT INTO logs VALUES (?, ?, ?)", msg.Time, string(msg.Value), "INFO")
    }
}
```

### Data storage

The log data is stored in ClickHouse, which is scaled to handle Zomato's requirements. Initially, the system utilized 10 M6g.16xlarge AWS EC2 nodes, but this configuration may evolve based on changing needs. The use of EC2 instances with high memory and compute capabilities ensures that ClickHouse can handle large volumes of data efficiently.

![ClickHouse Buckets](https://lh6.googleusercontent.com/-q42plmXWjIfx61IuQbqQKWMkfCgEzid0eZ-eAzMdhW167bYFdCnZdg7VC8F5-qOBz83udtg_NYtPOKXURxv1roj4CATYQuRgeUyNRqAxH4MXFdU4vrCXXLwcdxu7pAYo3QxC1U5NQQYhnxFZmALcMk)

### UI Application

A custom dashboard was developed for viewing and filtering logs. This dashboard is designed for high performance, achieving:

- **First Contentful Paint (FCP):** 0.95 seconds
- **Largest Contentful Paint (LCP):** 1.9 seconds

The dashboard utilizes modern web technologies and performance optimization techniques to ensure a responsive user experience. Features like lazy loading and efficient data fetching contribute to the dashboardâ€™s fast load times.

### Efficient data ingestion

1. **Batch Processing:** Instead of using ClickHouse Kafka plugins, Zomato opted for custom Golang workers to batch log entries. Each batch contains up to 20,000 entries, reducing the overhead on ClickHouse and maintaining a lag of less than 5 seconds. Batch processing minimizes the number of write operations and optimizes the use of system resources.

2. **Native Format Insertion:** Data is inserted using ClickHouseâ€™s native format, resulting in a performance boost and reduced I/O intensity compared to HTTP-based insertion. Native format insertion leverages ClickHouseâ€™s optimized binary protocols for efficient data transfer.

```-- Create a table for logs
CREATE TABLE IF NOT EXISTS logs (
    timestamp DateTime,
    message String,
    level LowCardinality(String)
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp);

-- Example of batch insertion using ClickHouseâ€™s native format
INSERT INTO logs (timestamp, message, level) VALUES
('2024-07-29 12:00:00', 'User logged in', 'INFO'),
('2024-07-29 12:01:00', 'Error processing request', 'ERROR'),
('2024-07-29 12:02:00', 'User logged out', 'INFO'),
('2024-07-29 12:03:00', 'System update completed', 'INFO');
```

3. **Round-Robin load distribution:** Workers use a round-robin strategy to distribute load evenly across ClickHouse nodes, ensuring efficient data handling and redundancy. This approach helps balance the load and prevent bottlenecks in the system.

```
// Pseudo-code for round-robin distribution
nodes := []string{"node1", "node2", "node3"} // List of ClickHouse nodes
currentNode := 0

for _, logEntry := range logEntries {
    node := nodes[currentNode]
    currentNode = (currentNode + 1) % len(nodes)

    // Insert data into the selected node
    insertIntoNode(node, logEntry)
}

func insertIntoNode(node string, logEntry LogEntry) {
    // Function to insert logEntry into the specified node
}
```

### Schema design

1. **Semi-Structured Tables:** Logs are stored in tables with common columns at the top level and other fields in a `Map<String, String>`. This design allows for flexible querying and efficient storage. The semi-structured approach facilitates easy adaptation to varying log formats and structures.

2. **Compression Optimization:** The schema uses special codecs and `LowCardinality` strings to enhance compression ratios and reduce storage requirements. Compression codecs such as Delta, Gorilla, and LZ4 are employed to achieve high compression rates.

```
-- Example schema with semi-structured tables and compression optimization
CREATE TABLE logs
(
    timestamp DateTime,
    message LowCardinality(String),
    level LowCardinality(String)
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp);
```

3. **Custom SDK:** Zomato developed an SDK to enforce structured logging practices, standardizing top-level fields to simplify log management and improve query efficiency. The SDK provides tools for consistent log formatting and integration with ClickHouse.

### Frontend dashboard

The custom dashboard for log management architecture includes:

1. **Lazy Loading:** Defers loading non-critical resources to improve initial load times. Lazy loading ensures that only essential components are rendered first, reducing the initial page load time.

2. **Filter Application and Date Range Picker:** Provides flexible log filtering and customization options. Users can filter logs by various criteria and select date ranges for analysis, enhancing the dashboard's usability.

3. **DOM Virtualization:** Renders only visible records to handle large data volumes efficiently. DOM virtualization improves performance by minimizing the number of DOM elements rendered at any given time.

### Replication and data management

1. **No Replication:** Zomato opted out of replication and Zookeeper to simplify operations. Data is backed up on S3 and EBS, allowing for rapid recovery in case of failures. The decision to skip replication was driven by the need for operational simplicity and cost savings.

2. **Data Tiering:** Implements TTL to move older data to cold storage and deletes data after 3 months. This approach helps manage storage costs and maintains system performance. Data tiering ensures that only recent and relevant data is kept in hot storage.

### Secondary indexes

```
-- Example of Bloom filter usage for optimizing query performance
CREATE TABLE logs
(
    timestamp DateTime,
    message String,
    level String
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp)
PRIMARY KEY (level)
INDEX idx_level (level) TYPE bloom_filter(0.1);
```

1. **TokenBF_v1 Index:** Improves query speed by bypassing non-matching data blocks, which is beneficial for large-scale data scans. TokenBF_v1 is a specialized index for efficiently handling high-cardinality data.

2. **Bloom Filters:** Used to quickly determine if a query might match a set of records, reducing unnecessary data reads. Bloom filters are probabilistic data structures that help in reducing the number of I/O operations.

### Ensuring resiliency

1. **Query Throttling:** Implements throttling mechanisms to handle high query loads, prioritizing efficient queries and maintaining system availability. Throttling ensures that the system can handle peak loads without compromising performance.

2. **Enhanced Read Query Niceness:** Optimizes resource allocation to prioritize write operations during system stress. The system dynamically adjusts resource allocation based on query types and workloads.

### Performance metrics

1. **Metrics collection:** Uses Prometheus to scrape ClickHouse metrics and Grafana for visualization and alerting. Prometheus collects time-series data on system performance, and Grafana provides dashboards for real-time monitoring.

2. **Performance Insights:** Monitors various health metrics and query performance to continuously optimize the system. Key metrics include query response times, resource utilization, and error rates.

### Security measures

1. **Authentication and Access Control:** Implements Google Authentication, table-level access control, and query auditing to secure log data and prevent unauthorized access. Google Authentication provides secure and scalable user authentication, while table-level access control ensures that users can only access data they are authorized to view.

### Improvements achieved

The migration to ClickHouse brought significant improvements:

1. **Reliability:** Achieved real-time data ingestion with less than 5 seconds of lag, supporting timely decision-making. The system's reliability has improved, allowing for near-instantaneous access to log data.

2. **Performance:** Reduced query times to P99 of 10 seconds, with complex queries completing in under 20 seconds. Query performance has been significantly enhanced, making it easier to analyze large volumes of data.

3. **Cost savings:** Potentially saved over a million dollars annually compared to the previous Elasticsearch setup. The cost-effectiveness of ClickHouse has resulted in substantial savings for Zomato.

## Conclusion

Zomato's transition to ClickHouse represents a strategic move to enhance their logging infrastructure, addressing scalability and cost issues associated with Elasticsearch. By leveraging ClickHouse's high-performance capabilities, custom design choices, and efficient data management practices, Zomato has significantly improved their logging system's reliability, performance, and cost-effectiveness.
